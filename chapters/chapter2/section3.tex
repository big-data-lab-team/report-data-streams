\section{Lossy Compression}

\TG{You should say here that you will use lossy compression and in particular
LTC in the future.}
% 在这个论文里，我们之关注Lossy compression method. 
% 因为基本上sensor 都会有一些误差 and 我们不许要 准确的数据，我们希望通过牺牲一定的精确度，增强我们的压缩ratio，而且我们的环境要求  尽可能少的使用 memory， 
% 因此 lossy compression 是我们主要focus 的算法。 
% \todo{需要说 为什么使用LTC，而不是其他}


\subsection{Lightweight Temporal Compression Algorithm}
\label{sec:ltc}

The Lightweight Temporal Compression (LTC)~\cite{schoellhammer2004lightweight}
algorithm approximates the data stream by a piece-wise linear function of time,
with an error bounded by parameter $\epsilon$.

The LTC algorithm maintains two lines, the \emph{high line}, and the \emph{low
line} defined by (1) the latest transmitted point and (2) the \emph{high point}
(high line) and the \emph{low point} (low line). When a point ($t_i$, $x_i$) is
received, the high line is updated as follows: if $x_i+\epsilon$ is below the
high line then the high line is updated to the line defined by the last
transmitted point and ($t_i$, $x_i+\epsilon$); otherwise, the high line is not
updated. Likewise, the low line is updated from $x_i-\epsilon$. Therefore, any
line located between the high line and the low line approximates the data points
received since the last transmitted point with an error bounded by
$\epsilon$~\cite{schoellhammer2004lightweight}. We assume the points on high
line are $(t_i, hp_i)$, and the points on low line are $(t_i, lp_i)$, where
$hp_i$ and $lp_i$ are the value of high line and low line at corresponding time
$t_i$.
The point $(t_{i-1}, \frac{hp_{i-1}+lp_{i-1}}{2})$ shell be transmitted if the
received point meets the condition: $x_i+\epsilon < lp_{i}$ or $x_i-\epsilon >
hp_{i}$. A reproduced example is presented in Figure~\ref{fig:ltc-review}. From
Figure~\ref{fig:ltc-review-b}, \emph{high line} and \emph{low line} is updated
when we receive point at time $t_2$ and $t_3$, but the condition $x_4+\epsilon <
lp_{4}$ is met when point $(t_4, x_4)$ come and we transmit point $(t_3,
\frac{hp_{3}+lp_{3}}{2})$.

\begin{figure}
    \centering
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=.8\columnwidth]{figures/LTC-a.png}
        \caption{}
        \label{fig:ltc-review-a}
    \end{subfigure}
    \centering
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=.8\columnwidth]{figures/LTC-b.png}
        \caption{}
        \label{fig:ltc-review-b}
    \end{subfigure}
    \caption{Lightweight temporal compression example, extracted from~\cite{zordan2014performance}}
    \label{fig:ltc-review}
\end{figure}



\subsection{Piece-wise Linear Approximation
with Minimum number of Line Segments Algorithm}

\TG{??You should also add a list of acronyms to the document}

Similar to LTC, Piece-wise Linear Approximation with Minimum number of Line
Segments Algorithm (PLAMLis)~\cite{liu2007energy} represents the original stream
through a sequence of line segments. The main idea of this algorithm is to
represent the stream data over a time window using a minimum number of segments
so that amount of data transferred is reduced. PLAMLis gives a greedy algorithm
solution. Assume the input stream data points $X=\{x_1, ..., x_W\}$ are received
over a time window of size $W$. Firstly, for each data points $x_i$, $i \in 
\{1, ..., W\}$, a longest segment $S_{i}$ from point $x_i$ to point $x_j$
($j>i$) is built within the error bound. Thereby for the data points in the
window, a sequence of longest segments $S = {S_1, ..., S_W}$ is obtained.
Secondly, to pick the minimum number of subsets of S for representing original
stream $X$, a greedy algorithm is used to select the segment $S_k$ ($k \in
\llbracket1, W\rrbracket$) which covers the largest number of data points $x_i$
in $X$ at each time, then remove it from $S$ and add it into a \texttt{result
sequence} until all data points in $X$ are covered~\cite{liu2007energy}. The
result sequence is the result of compression~\cite{zordan2012compress,
zordan2014performance}.


\subsection{Enhanced Piece-wise Linear Approximation with Minimum number of Line
Segments Algorithm} 

Enhanced Piece-wise Linear Approximation with Minimum number of Line Segments
Algorithm (Enhanced PLAMLis)~\cite{pham2008enhance} solves the problem "to
represent stream data over a time window through using minimum number of
segments" with a top-down recursive segmentation algorithm which has smaller
computational cost than PLAMLis~\cite{pham2008enhance, zordan2014performance}.
Assume $W$ data points $x_i$ in time window, the segment $S_{(1, W)}$ with end
points $x_1$ and $x_W$ is created, then checking whether the maximum error is
within error tolerance $\epsilon$ determines stopping the recursive or not. If
the maximum error is bigger than $\epsilon$, the segment is split into two
shorter segments $S_{(1, k)}$ and $S_{(k, W)}$ in data point $x_k$, $1<k<W$.
Applying this procedure recursively on each segment until the maximum error of
all segments is within the error tolerance~\cite{pham2008enhance,
zordan2014performance}. 

\TG{How do these algorithms compare to LTC?}


\todo{Try to add more content}
\subsection{Polynomial Regression}
\label{sec:polynomial}

Different from piece-wise linear approximations, Polynomial
Regression~\cite{zordan2014performance} gives a higher order $p \geqslant1$
approximation by using standard regression methods based on least squares
fitting~\cite{phillips2003interpolation}. The approximation is a sequence of
curves (order = $p$) rather than linear segments. The algorithm starts with
collecting $p+1$ samples $\{x_1, ..., x_{p+1} \}$ to obtain the coefficients of
first $p$-order polynomial function. Upon receiving one sample $x_{p+1+i}$ at
each time, where $x_{p+1+i}$ indicates the $(p+1+i)_{th}$ sample ($i>0$) in this
approximation cycle, the best-fitting polynomial coefficients are re-computed
with $\{ x_1, ..., x_{p+1+i}\}$ and the algorithm checks whether the new
polynomial approximates the data points within the desired error tolerance. If
not, the coefficients of the previous regression are transmitted and a new
approximation starts at the current sample~\cite{zordan2014performance}. 

During the compression process, all the points between transmissions need to be
kept in memory, and the least squares fitting required larger computational cost
than piece-wise linear approximations. However, polynomial regression gives
better performance in term of Root Mean Square Error (RMSE) between
reconstructed data and original data. It means that the result from regression
method is closer original data than result from piece-wise linear approximation
method~\cite{zordan2014performance}.

% \begin{algorithm}
% \begin{algorithmic}[1]
% \Input
%     \Desc{$\chi$}{Received data stream}
%     \Desc{$\epsilon$}{Error bound}
%     \Desc{$p$}{The order of polynomial function}
% \EndInput
% \Output
%     \Desc{tr}{Transmitted coefficients}
% \EndOutput

% \State $S$ = $\O$
% \State $k$=1; $j$=0
% \While{True}
%     \State $S = S \cup \chi$
%     \If{$i \geqslant p+1$}
%         \State j += 1
%         \State $M_j^k$ = model($S$, $p$)    \Comment{Compute coefficients}
%         \ForAll{$x_i \in S$ and $\hat{x}_i \in$ predict($M_j^k$)}   \Comment{Check if error bound is met}
%             \If{$|\hat{x}_i - x_i| > \epsilon$}
%                 \State tr = $M_{j-1}^k$ \Comment{Transmit coefficients}
%                 \State k += 1; j = 0
%                 \State $S$ = $\chi$
%             \EndIf
%         \EndFor
%     \EndIf
% \EndWhile
% \end{algorithmic}
% \caption{Polynomial Regression Algorithm}
% \label{algo:polynomial}
% \end{algorithm}

\subsection{Adaptive Auto-Regression Moving-Average technique}


Adaptive Auto-Regression Moving-Average (A-ARMA)~\cite{lu2010optimized} is a
improved version of Auto-Regression Moving-Average (ARMA). ARMA model is formed
by combining AR and MA model, and it usually used as  a tool to predict future
values over time series data~\cite{chatfield2016analysis}. ARMA model
\texttt{ARMA($p$, $q$)}, containing $p$ AR terms and $q$ MA terms, is defined
as:
\begin{equation}
X_t = \sum_{i=1}^{p}a_{i}X_{t-i} + Z_t + \sum_{i=1}^{q}\beta_{i}Z_{t-i}
\end{equation}
\noindent The equation reproduced from~\cite{chatfield2016analysis}, where 
$a_1, ..., a_p$ and $\beta_1, ..., \beta_q$ are parameters of AR model and MA
model respectively, $Z_{t-q}, ...,Z_{t}$ are white noise (usually understand as
residuals of the previous forecasts, $Z_t = X_t -
X_{t-1}$)~\cite{chatfield2016analysis}.

Similar to the ARMA model, A-ARMA is also composed of
two terms, \texttt{Auto-Regression (AR)} term and \texttt{Moving-Average (MA)}
term, respectively predicting data value using $p$($q$) prior values or errors. 
To deal with the limit of computational complexity, A-ARMA adopts low-order ARMA
with sliding window model~\cite{lu2010optimized}. The main idea of A-ARMA is
maintaining and updating a ARMA model in memory based on sliding window. 

Let's assume $W$ is a sliding window with $W$ window size, $th_{err}$ is the
minimum error tolerance on root-mean-square error (RMSE) and $S$ means the
length of each movement of sliding window. The adapted algorithm of A-ARMA is
given in Algorithm~\ref{algo:A-ARMA}. $model_{(p, q)}$ is the parameters of
ARMA($p$, $q$) model, obtaining through function \texttt{build\_ARMA()}.
Function \texttt{go\_forward()} makes the sliding window $W$ to move $S$ length
(read $S$ data samples), and function \texttt{tail($S$)} returns $S$ samples at
the end of the window. RMSE is calculates by function \texttt{compute\_error()}.

The first $W$ data points are used to initialize the ARMA model, and to compare
the RMSE between the original and predicted subsequent $S$ data by moving
sliding window $S$ length each time. If the RMSE is larger than $th_{err}$, the
saved ARMA model is remodeled with the current samples in sliding
window~\cite{lu2010optimized}.
In the decompression process, the stream data are predicted based on the
parameters transmitted.

\begin{algorithm}
\begin{algorithmic}[1]
\Input
    \Desc{$stream$}{$\quad \quad \quad $Data stream received}
    \Desc{$W$}{$\quad \quad \quad $Sliding window}
    \Desc{$th_{err}$}{$\quad \quad $Threshold of error tolerance on root-mean-square error}
    \Desc{$S$}{$\quad \quad \quad $Length of sliding window move}
    \Desc{$p$}{$\quad \quad \quad $Order of AR term}
    \Desc{$q$}{$\quad \quad \quad $Order of MA term}
\EndInput
\Output
    \Desc{$model_{(p, q)}$}{$\quad \quad \quad $Parameters of ARMA($p$, $q$) model}
\EndOutput

\State Read stream till $W$ is full \Comment{Get first $W$ data from $stream$}
\State $model_{(p, q)}$ = build\_ARMA($W$.samples, $p$, $q$)  \Comment{Build ARMA model}
\While{$stream$ is not empty}
    \State $W$.go\_forward($S$) \Comment{Moving sliding window forward $S$ length}
    \State $samples$ = $W$.tail($S$)    
    \State $RMSE$ = compute\_error($samples$,  $model_{(p, q)}$.predict())
    \If{$RMSE > th_{err}$}
        \State $model_{(p, q)}$ = build\_ARMA($W$.samples, $p$, $q$)
        \State \Return $model_{(p, q)}$ 
    \Else
        \State \Return null \Comment{No transmitted data, model does not change}
    \EndIf
\EndWhile
\end{algorithmic}
\caption{A-ARMA algorithm, adapted from~\cite{lu2010optimized}}
\label{algo:A-ARMA}
\end{algorithm}

% \noindent At first, Let's define several parameters:
% \begin{itemize}
%     \item $W$ is the window size. The parameters of the ARMA
%     model are computed on the latest $W$ samples.
%     \item $th_{err}$ is the threshold of the error tolerance on the root-
%     mean-square (RMS) error between predicted values and
%     actual data. It also represents the accuracy that the A-
%     ARMA model achieves locally over each window.
%     \item $S$ is the step size based on which the computational
%     window moves forward.
% \end{itemize}
% \noindent The A-ARMA model estimation on a sensor node works as follow:
% \begin{enumerate}

%     \item It builds an ARMA($p$, $q$) model once it has collected
%     $W$ samples, where $p + 1$ parameters for the AR part and
%     $q$ parameters for the MA part are computed.
%     \item Upon collecting the next $S$ samples, the node measures
%     the RMS error between predicted values and the actual
%     values in the current window.
%     \item If the difference is within $th_{err}$, then the node continues
%     using its current ARMA model.
%     \item Else (i.e., the difference is greater than $th_{err}$), then the
%     node re-computes the new parameters for the ARMA
%     model on the most recent W samples. The new parameters
%     of the ARMA model are transmitted.
% \end{enumerate}

\subsection{Modified Adaptive Auto-Regression}

Modified Adaptive Auto-Regression (MA-AR) is a modified version of A-ARMA,
proposed by Zordan et al.~\cite{zordan2012compress}. In the A-ARMA algorithm,
the ARMA model is built or updated over fixed window of $W$ samples. It might
cause bad performance to predict next $S$ samples with trained ARMA model over a
fixed window, especially in highly noisy environments~\cite{zordan2012compress}.
Assuming the prediction cycle means a process to find a AR model which represent
as mush original data as possible within error tolerance. The MA-AR algorithm
uses a $p$-order AR model for each prediction cycle instead of sliding window,
and controls the absolute error on each data rather than RMSE of $S$ continuous
data. Assume $M^{(n, i)}$ indicates the AR model built according to data 
$\{x_n, ..., x_{n+p-1+i} \}$, where $i>0$, and $\hat{x}_{n+p-1+i}$ indicates the
predicted data, then for each prediction cycle, MA-AR works as follows:

\begin{enumerate}
    \item Collect first $p$ samples in sensor node and send them to client side.
    \item Collect one sample $x_{n+p-1+i}$ at a time, $i > 0$, to build
    $p$-order
    AR model $M^{(n, i)}$.
    \item Predict $x_{n+p-1+j}$ where $j \in \{1, ..., i\}$ using $M^{(n, i)}$.
    \item Check whether error $ |\hat{x}_{n+p-1+j} - x_{n+p-1+j}|$ is larger
    than error tolerance $\epsilon$.
        \begin{itemize}
            \item If $|\hat{x}_{n+p-1+j} - x_{n+p-1+j}| \leqslant \epsilon$, the
            model is kept. Repeat from step 2.
            \item Else the last model $M^{(n, i-1)}$ is encoded and transmitted,
            and new predict cycle is started from $x_{n+p-1+i}$.
        \end{itemize} 
\end{enumerate}
The main idea of this algorithm is continuous estimation of the AR parameters.
AR model is redefined only according to last coming sample, so the computational
cost is minimized and the parameters of model can be computed through least
squares minimization~\cite{zordan2012compress}. 
