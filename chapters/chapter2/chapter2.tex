\chapter{Related Work}
\section{Stream summarization}

Stream data has became more and more important since the rist of the Internet of
Thing. Devices in IoT communicates with stream data. Stream data is transmitted
at high speed, and short intervals. Hardward also has limited small storage, so
stream data may never be reviewed if we did not process it immediately. Some
summarization technonic has been researched and proposed due to its volume and
velocity. We introduces several steam summarization techniques in this section.

\subsection{Overview of Stream Problems and Algorithms}
Most data stream analysis algorithms rely on data reduction and synopsis
construction techniques to compute approximate solutions that work with lim-ited
storage and time. We introduce some data reduction and sysnopsis construction
thechniques following:
\begin{quote}
\begin{itemize}
    \item  \textbf{Sampling} captures sub-sample of a data stream to represent
    data whole data stream and it could be used for extracting essential
    characteristics of a data stream~\cite{kejariwal2015real}.
    
    \item \textbf{Filtering} eliminate unuseful or unwanted data points from
    stream. We can only capture the data which we are intersted in. A sample
    example is filtering spam email.
    
    \item \textbf{Sliding windows} maintains a windows which moves with new data
    coming, it ensures the analysis and statistics using fresh data. It also 
    often uses for recording data in given bounded memory.

    \item \textbf{Frequency Moments} shows characteristics of the stream.
    Moments involves the distribution of frequencies of different elements in
    the stream. 
    
    
    \item \textbf{Clustering} separate all data points into different groups.
    Clustering is a techniques used in Mechine Learning. It could make the 
    similar points to cluster in one group. It makes sure that the points in one
    group as similar as possible, the points in different group as different as
    possible. For instance, they can be applied to problems such as the k-median 
    problem. 
    
    \item \textbf{Sketches}~\cite{alon1999space} summarizes data stream with 
    small amount memory. The sketch is usually used for specific queries over 
    data set.~\cite{kejariwal2015real}

\begin{comment}
    \item \textbf{Histograms} approximate the distribution of a set of values
    $v_1, ..., v_n$ by a piecewise constant function $\hat{v}(i),$ so as to
    minimize the sum of squared error. Equi-width histograms partition the
    domain into buckets such that the number of $v_i$ values falling into each
    bucket is uniform across all buckets. End-biased histograms maintain exact
    counts of items that occur with frequency above a threshold, and approximate
    the other counts by a uniform distribution~\cite{kejariwal2015real}.
    
    \item \textbf{Wavelet coefficients} are projections of the given signal (set
    of data values) onto an orthogonal set of basis vectors. The coefficients
    have the desirable property that the signal reconstructed from the top few
    wavelet coefficients b est approximates the original signal in terms of the
    L2 norm~\cite{gilbert2002fast}. The choice of basis vectors determines the
    type of wavelets.
\end{comment}

\end{itemize}
\end{quote}

For each problem in streaming research, there are many algorithms to solve or
estimate it. The remainder of this chapter presents certain algorithms used in
specific streaming analysis problem.

\subsection{Sampling Stream}

When the data is huge and we cannot save it into working storage, so we would to
sampling data from the whole data stream, then estimating the result base on
fraction of the sample and whole data.But there are some trick in it. There is a
instance from~\cite{leskovec2014mining}, we would like to query "What fraction
of the typical user's queries were repeated over the past month?". The sampling
would be used when data is huge. The common idea is to extract 1/10th data from
the whole stream, each data has 10\% probability be selected. Assume a user
issues N search queries in past month, only D search queries twice, other
queries issue once. The queries appear twice after sampling is$\frac{D}{100}$,
because the probability of a data appears twice in sample
is$\frac{1}{10}*\frac{1}{10}$(first time appear is $\frac{1}{10}$, the second
appear also need $\frac{1}{10})$. $\frac{18D}{100}$ repeat queries appear only
once in samples($2*\frac{1}{10}*\frac{9}{10}$). The result of this idea is
wrong. The correct answer is $\frac{D}{N}$. However, the answer we obtain after
sampling is:
\begin{equation*}
    \frac{\frac{D}{100}}{\frac{N-D}{10}+\frac{18D}{100}+\frac{D}{100}} = \frac{D}{10N+9D}
\end{equation*}

Another idea is to extract the queries from 1/10th users. Using a hash
function(result 0-9), the queries are accepted for the sample, if the the users
hashes to 0. In this instance, we take $user$ as the key. 

So, in general sample problem, to take a $\frac{a}{b}$ size sample from whole
stream, could hash the key value to $b$ buckets, and accept data for sample
while the hash value less than $a$.~\cite{leskovec2014mining}

In practice, sampling can help to save energy on connected objects. For
instance, when we would to do some query on stream, we can estimate the result
after sampling from whole stream. We need less communication with objects,
because we only need just a part of whole stream to estimate result.  

\subsection{Filtering Stream}
Some times, we are interested only in particular elements in the stream. For
instance, some connected objects which have sensor producing longitude and
latitude send the stream data to client side. In this case, if we only prefer
the data from specific longitude and latitude, filtering stream is needed.
Another instanceï¼Œwe are receiving a list of email addresses which include lots
of spam. Therefore, we need to filter the steam and focus on the non-spam
addresses. Assume we have too large size email list to store in device which has
limited main memory. The technique knows as \textbf{Bloom
filtering}~\cite{bloom1970space} could be used. 
\paragraph{Bloom Filter}

Suppose for main memory, we have $n$ bits for filtering, $m$ non-spam email
addresses in list $S$, and $k$ hash functions. Each hash function maps the email
address into $n$ buckets. Initializing an array of bits of $n$ size as follow: 

\begin{enumerate}
  \item Initialize an array of bits $N$ by setting 0's in $n$ bits.
  
  \item Updating array of bits, for each email address in $S$, hashing with all
  hash functions and changing the corresponding bit to 1's based on hash result.
\end{enumerate}

After initialization, Each new coming stream data will be filtered by using the
array of bits $N$ and $k$ hash functions. When a new stream data coming, we can
get a bits result from those hash functions. Adding this data for sample if the
result are all 1's, otherwise filtering it.  But, the email not in $S$, might
pass the filter too, and be selected for sample. We would like to calculate the
probability of a spam email be selected for sample. Obviously, the spam emails
are harder to pass, if the fraction of 0 bits in bits array$n$. During the stage
of initialize bits array, we have $m$ emails in $S$, and $k$ hash functions. For
each emails belongs to $S$, suppose hash result corresponds target bit. The
probability of each hash result hit given bit is $\frac{1}{n}$, not hit given
bit is $\frac{n-1}{n}$. Because we have $k$ hash function, the probability that
one emails not hit given bit is $(\frac{n-1}{n})^k$. After hashing $m$ emails,
the probability that a bit in bits array still remains 0 is
$(\frac{n-1}{n})^{km}$, which equals  $(1-\frac{1}{n})^{n(\frac{km}{n})}$. It
approximate equals $e^{-\frac{km}{n}}$, using the approximation
$(1-\epsilon)^\frac{1}{\epsilon} = \frac{1}{e}$. For a new coming spam stream
data, the probability that it is selected for sample is
$(1-e^{-\frac{km}{n}})^k$, which means all hash result mapping to bits are 1's.



\subsection{Sliding Window}

\subsection{Estimating Cardinality}

\subsection{Clustering}

\subsection{Sketches}

\section{Lossless Compression}
\subsection{LZ77}
\subsection{LZSS}
\subsection{A-LZSS}

\subsection{LEC}
\subsection{S-LEC}

\subsection{LZW}
\subsection{S-LZW}



\section{Lossy Compression}
