\section{Introduction}

% introduce Streaming data
Streaming data has become more and more important since the rise of the Internet
of Things. Devices in IoT communicate using streaming data, transmitted at high
speed and short intervals, which may never be reviewed if the system does not
process or store it immediately. To decrease the amount of data transmitted,
stream summarization is useful and effective. Summarization can be considered as
a process to discover a compressed description of the original data-set with the
lowest possible \emph{information loss}~\cite{chandola2007summarization}.
% Summarization is a key data mining concept which involves techniques for finding a compact description of a data-set. Summarization can be viewed as compressing a given set
% of transactions into a smaller set of patterns while retaining the maximum possible information~\cite{chandola2007summarization}. \todo{change}

Some stream summarization techniques are used for data reduction, for instance,
\emph{Sampling} techniques, including uniform random
sampling~\cite{vitter1984faster, Ahrens1985SequentialRS}, Reservoir
sampling~\cite{vitter1985random, Aggarwal2007DataS} and weighted
sampling~\cite{chaudhuri1999random, efraimidis2006weighted}, capture a
sub-sample of data stream to represents the entire stream; \emph{Filtering}
techniques help us helps us to omit the items in the stream. Bloom
filter~\cite{bloom1970space} and Cuckoo filter~\cite{fan2014cuckoo} are
well-known methods and have been applied in many fields. we are able to obtain
the information or data that we are interested and answer the query which
requires the specific part of entire data stream by using filtering. In
addition, to  answer queries over data stream, computing approximate result is
more suitable for any queries than exact solution~\cite{kejariwal2015real}.
Previous research has provided some synopsis constructions for summarization.
\emph{Histogram} techniques~\cite{kejariwal2015real, ahmed2019data} which give a
distribution of a items in stream. \emph{Sketch} techniques are able to solve
some specific problem, such as, the number of distinct items in stream can be
estimated by using Flajolet and Martin sketch
(FM-Sketch)~\cite{flajolet1985probabilistic, garofalakis2016data}, the second
frequency moment of stream is able to be solved through Alon-Matias-Szegedy
Sketch (AMS Sketch)~\cite{alon1999space}, and Count-Min
sketch~\cite{cormode2005improved, garofalakis2016data} can be used to calculate
the quantiles and frequent items in stream roughly. Moreover, many summarization
methods applies \emph{Sliding window}~\cite{datar2002maintaining} technique
which maintain a window that moves with new data coming. it ensures that the
methods always use fresh data for analysis and statistics by keeping the most
recent items of stream or all items within specific time period in given bound
memory~\cite{leskovec2014mining}. However, many summarization techniques only
focus on specific problems and might eliminate the most or partial data
information. In some case, the integrity of data information and accuracy of
query answers are required, thus we need a summarization technique which can
both reduce size of stream and  keep integral data information. Data compression
is such a technique which meets above conditions. 

% \todo{a transition to compression}
% \TG{This section is still too long: we don't know why we have to read the previous 3.5 pages. These pages should be
% summarized in 1-2 paragraphs, with a transition to compression at the end.}
% introduce compression

Data compression is a technique to reduce the number of bits required to
represent a data set. It is also considered as a summarization technique
which can give compact version of the entire original
data~\cite{hesabi2015data}. Data compression is categorized into
\emph{lossless} and \emph{lossy} compression:
\begin{itemize}
    \item \emph{Lossless} compression methods remove statistical redundancy and
    the original data can be retrieved through decompression without any
    information loss~\cite{hesabi2015data}.
    \item \emph{Lossy} compression methods omit some information in the original
    data, but ensure that the reconstructed data has certain accuracy. For lossy
    compression, there is a trade-off between reconstruction accuracy and
    additional gains in terms of compression ratio~\cite{zordan2014performance}.
\end{itemize}

In our case, the compression technique is the best choice of data summarization
to reduce the rate of radio transmission, because losing any one data points in
data stream is not allowed during transmission. We can obtain the original data
stream through decompression process. Moreover, lossy compression methods might
are suitable for sensor data streams than lossless methods, because measured
sensor data intrinsically involves noise and measurement errors, which can be
treated as a configurable tolerance for a lossy compression
algorithm~\cite{li2018multi}. 

Resource-intensive lossy compression algorithms such as the ones based on
polynomial interpolation, discrete cosine and Fourier transforms, or
auto-regression methods~\cite{lu2010optimized} are not well-suited for
connected objects, due to the limited memory available on these systems
(typically a few KB), and the energy consumption associated with CPU
usage~\cite{li2018multi}. Instead, compression algorithms need to find a
trade-off between reducing network communications and increasing memory and CPU
usage. As discussed in~\cite{zordan2014performance}, linear compression methods
provide a very good compromise between these two factors, leading to
substantial energy reduction~\cite{li2018multi}. In this chapter, we will
review several lossless compression methods in section~\ref{sec:lossless} and
lossy compression methods in section~\ref{sec:lossy}.
