\section{Introduction}

% introduce Streaming data
Streaming data has become more and more important since the rise of the Internet
of Things. Devices in IoT communicate using streaming data, transmitted at high
speed and short intervals, which may never be reviewed if the system does not
process or store it immediately. To decrease the amount of data transmitted,
stream summarization is useful and effective. Summarization can be considered as
a process to discover a compressed description of the original data-set with the
lowest possible \emph{information loss}~\cite{chandola2007summarization}.
% Summarization is a key data mining concept which involves techniques for finding a compact description of a data-set. Summarization can be viewed as compressing a given set
% of transactions into a smaller set of patterns while retaining the maximum possible information~\cite{chandola2007summarization}. \todo{change}

Some stream summarization techniques are used for data reduction, for instance,
\emph{Sampling} techniques, including uniform random
sampling~\cite{vitter1984faster, Ahrens1985SequentialRS}, Reservoir
sampling~\cite{vitter1985random, Aggarwal2007DataS} and weighted
sampling~\cite{chaudhuri1999random, efraimidis2006weighted}, capture a
sub-sample of data stream to represents the entire stream; \emph{Filtering}
techniques help us helps us to omit the items in the stream. Bloom
filter~\cite{bloom1970space} and Cuckoo filter~\cite{fan2014cuckoo} are
well-known methods and have been applied in many fields. we are able to obtain
the information or data that we are interested and answer the query which
requires the specific part of entire data stream by using filtering. In
addition, to  answer queries over data stream, computing approximate result is
more suitable for any queries than exact solution~\cite{kejariwal2015real}.
Previous research has provided some synopsis constructions for summarization.
\emph{Histogram} techniques~\cite{kejariwal2015real, ahmed2019data} which give a
distribution of a items in stream. \emph{Sketch} techniques are able to solve
some specific problem, such as, the number of distinct items in stream can be
estimated by using Flajolet and Martin sketch
(FM-Sketch)~\cite{flajolet1985probabilistic, garofalakis2016data}, the second
frequency moment of stream is able to be solved through Alon-Matias-Szegedy
Sketch (AMS Sketch)~\cite{alon1999space}, and Count-Min
sketch~\cite{cormode2005improved, garofalakis2016data} can be used to calculate
the quantiles and frequent items in stream roughly. Moreover, many summarization
methods applies \emph{Sliding window}~\cite{datar2002maintaining} technique
which maintain a window that moves with new data coming. it ensures that the
methods always use fresh data for analysis and statistics by keeping the most
recent items of stream or all items within specific time period in given bound
memory~\cite{leskovec2014mining}. However, many summarization techniques only
focus on specific problems and might eliminate the most or partial data
information. In some case, the integrity of data information and accuracy of
query answers are required, thus we need a summarization technique which can
both reduce size of stream and  keep integral data information. Data compression
is such a technique which meets above conditions. 

% \todo{a transition to compression}
% \TG{This section is still too long: we don't know why we have to read the previous 3.5 pages. These pages should be
% summarized in 1-2 paragraphs, with a transition to compression at the end.}
% introduce compression

Data compression is a technique to reduce the number of bits required to
represent a data set. It is also considered as a summarization technique
which can give compact version of the entire original
data~\cite{hesabi2015data}. Data compression is categorized into
\emph{lossless} and \emph{lossy} compression:
\begin{itemize}
    \item \emph{Lossless} compression methods remove statistical redundancy and
    the original data can be retrieved through decompression without any
    information loss~\cite{hesabi2015data}.
    \item \emph{Lossy} compression methods omit some information in the original
    data, but ensure that the reconstructed data has certain accuracy. For lossy
    compression, there is a trade-off between reconstruction accuracy and
    additional gains in terms of compression ratio~\cite{zordan2014performance}.
\end{itemize}

In our case, the compression technique is the best choice of data summarization
to reduce the rate of radio transmission, because losing any one data points in
data stream is not allowed during transmission. We can obtain the original data
stream through decompression process. 

Most of lossless compression methods belong to entropy coding or dictionary
coder, the main idea of them is the ``statistical model'' or ``dictionary''
generated according to input data. The ``statistical model'' or ``dictionary''
would be used to compress new data point into a bit sequences.  Many existing
literature has been devoted to the study of lossless compression. Huffman
coding~\cite{huffman1952method} and arithmetic
coding~\cite{langdon1984introduction} are the primary and classical entropy
coding methods. The Lossless Entropy Compression (LEC)
algorithm~\cite{marcelloni2008simple} is a approximated version of
exponential-Golomb code~\cite{teuhola1978compression}. In
~\cite{marcelloni2008simple}, LEC is utilized for compressing temperature and
humidity streams by using a very small dictionary whose size is determined by
the number of bits after analog-to-digital converter
(ADC)~\cite{marcelloni2008simple,marcelloni2009efficient}. The Sequential
Lossless Entropy Compression (S-LEC)~\cite{li2016temporal} algorithm is an
extension and improvement of LEC algorithm. It exploits the positional
relationship of groups of adjacent residues, in order to increase the
compression ratio. In dictionary-based lossless compression algorithms,
Lempel-Ziv-77 (LZ77)~\cite{ziv1977universal} and Lempel-Ziv-78
(LZ78)~\cite{ziv1978compression} are the well-known algorithms, where LZ77
maintains a sliding window as dictionary during compression. There are several
variations of LZ77 and LZ78, for instance, Lempel-Ziv-Welch
(LZW)~\cite{sadler2006data}, Lempel-Ziv-Storer-Szymanski
(LZSS)~\cite{storer1982data}, Lempel-Ziv-Oberhumer (LZO)~\cite{lzocite} etc. In
order to suit these algorithms to connected objects and data streams, many
algorithms improved are proposed, such as
Accelerometer-Lempel-Ziv-Storer-Szymanski
(Accelerometer-LZSS)~\cite{pope2018accelerometer} which combines LZSS and
Huffman coding to compress accelerometer data, and Sensor Lempel-Ziv-Welch
(Sensor LZW)~\cite{sadler2006data} algorithm which adapts LZW to sensor nodes.

Moreover, lossy compression methods might are suitable for sensor data streams
than lossless methods, because measured sensor data intrinsically involves noise
and measurement errors, which can be treated as a configurable tolerance for a
lossy compression algorithm~\cite{li2018multi}. Thus, In this thesis, we only
focus on lossy compression methods.

Resource-intensive lossy compression algorithms such as the ones based on
polynomial interpolation, discrete cosine and Fourier transforms, or
auto-regression methods~\cite{lu2010optimized} are not well-suited for connected
objects, due to the limited memory available on these systems (typically a few
KB), and the energy consumption associated with CPU usage~\cite{li2018multi}.
Instead, compression algorithms need to find a trade-off between reducing
network communications and increasing memory and CPU usage. As discussed
in~\cite{zordan2014performance}, linear compression methods provide a very good
compromise between these two factors, leading to substantial energy
reduction~\cite{li2018multi}. In this chapter, we will review several lossy
compression methods in Section~\ref{sec:lossy}.
