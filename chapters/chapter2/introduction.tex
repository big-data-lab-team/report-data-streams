\section{Introduction}

% introduce Streaming data
Streaming data has become increasingly important since the rise of the
\acrlong{iot}. Devices in the \acrshort{iot} communicate using streaming data,
transmitted at high speed and short intervals, which may never be reviewed if
the system does not process or store it immediately. To decrease the amount of
data transmitted, stream summarization is useful and effective. Summarization
can be considered as a process to discover a compressed description of the
original data-set with the lowest possible \emph{information
loss}~\cite{chandola2007summarization}.
% Summarization is a key data mining concept which involves techniques for finding a compact description of a data-set. Summarization can be viewed as compressing a given set
% of transactions into a smaller set of patterns while retaining the maximum possible information~\cite{chandola2007summarization}. \todo{change}

Some stream summarization techniques are used for data reduction. For instance,
\emph{Sampling} techniques, including uniform random
sampling~\cite{vitter1984faster, Ahrens1985SequentialRS}, Reservoir
sampling~\cite{vitter1985random, Aggarwal2007DataS} and weighted
sampling~\cite{chaudhuri1999random, efraimidis2006weighted}, capture a
sub-sample of the data stream to represents the entire stream. \emph{Filtering}
techniques reduce the number of items in the stream: Bloom
filter~\cite{bloom1970space} and Cuckoo filter~\cite{fan2014cuckoo} are
well-known methods and have been applied in many fields. 
% \english{we are able to obtain
% the information or data that we are interested in and answer the query which
% requires the specific part of the entire data stream by using filtering.
% \TG{sentence looks out of place}}
In addition, to  answer queries over a data stream, computing approximate result
is more suitable for any queries than the exact
solution~\cite{kejariwal2015real}. Previous research has provided some synopsis
constructions for summarization. \emph{Histogram}
techniques~\cite{kejariwal2015real, ahmed2019data} that give a distribution of
items in the stream. \emph{Sketch} techniques are able to solve some specific
problem, for instance, \acrfull{fm-sketch}~\cite{flajolet1985probabilistic,
garofalakis2016data} can solve the problem of finding the number of distinct
elements, \acrfull{ams-sketch}~\cite{alon1999space} is able to estimate the
second frequency moment, and Count-Min Sketch~\cite{cormode2005improved,
garofalakis2016data} can calculate the quantiles and find frequent items.
% \todo{Sketch sentence too long}
% for instance, the number of distinct items in a stream can be estimated by
% using \acrfull{fm-sketch}~\cite{flajolet1985probabilistic, garofalakis2016data},
% the second frequency moment of a stream is able to be solved through
% \acrfull{ams-sketch}~\cite{alon1999space}, and Count-Min
% Sketch~\cite{cormode2005improved, garofalakis2016data} can be used to calculate
% the quantiles and frequent items in a stream roughly \TG{This sentence is way
% too long}. 
Moreover, many summarization methods apply \emph{Sliding
window}~\cite{datar2002maintaining} technique which maintains a window that
moves with new data coming. It ensures that the methods always use fresh data
for analysis and statistics by keeping the most recent items of the stream or
all items within a specific time period in given bound
memory~\cite{leskovec2014mining}. However, many summarization techniques only
focus on specific problems and might eliminate most or partial data information.
In some case, the integrity of data information and accuracy of query answers
are required, thus we need a summarization technique which can both reduce the
size of the stream and keep integral data information. Data compression is such
a technique that meets the above conditions.

% \todo{a transition to compression}
% \TG{This section is still too long: we don't know why we have to read the previous 3.5 pages. These pages should be
% summarized in 1-2 paragraphs, with a transition to compression at the end.}
% introduce compression

Data compression is a technique to reduce the number of bits required to
represent a data set. It is also considered as a summarization technique
that can give a compact version of the entire original
data~\cite{hesabi2015data}. Data compression is categorized into
\emph{lossless} and \emph{lossy} compression:
\begin{itemize}
    \item \emph{Lossless} compression methods remove statistical redundancy and
    the original data can be retrieved through decompression without any
    information loss~\cite{hesabi2015data}.
    \item \emph{Lossy} compression methods omit some information in the original
    data, but ensure that the reconstructed data has certain accuracy. For lossy
    compression, there is a trade-off between reconstruction accuracy and
    additional gains in terms of compression ratio~\cite{zordan2014performance}.
\end{itemize}

In our case, the compression technique is the best choice of data summarization
to reduce the rate of radio transmission, because losing any data points after
reducing data stream is undesired, and compression guarantees the integrity of
data stream. In other words, we can obtain the original data stream through the
decompression process, thus no data points will be lost.

Most of the lossless compression methods belong to entropy or dictionary coding.
Their main idea is to represent the new data points based on the ``statistical
model'' or ``dictionary'', generated according to the data points we have seen.
% \todo{Be more explicit: a statistical model isn't an idea.}.
In general, the ``statistical model'' and ``dictionary'' help us to map the data
points into bit sequences, thus compressing the data set. Huffman
coding~\cite{huffman1952method} and arithmetic
coding~\cite{langdon1984introduction} are the primary and classical entropy
coding methods. \acrfull{lec} algorithm~\cite{marcelloni2008simple} is a
approximated version of exponential-Golomb code~\cite{teuhola1978compression}.
In ~\cite{marcelloni2008simple}, \acrshort{lec} is utilized for compressing
temperature and humidity streams by using a very small dictionary whose size is
determined by the number of bits after
\acrfull{adc}~\cite{marcelloni2008simple,marcelloni2009efficient}. The
\acrfull{s-lec}~\cite{li2016temporal} algorithm is an extension and improvement
of \acrshort{lec} algorithm. It exploits the positional relationship of groups
of adjacent residues, in order to increase the compression ratio.

In dictionary-based lossless compression algorithms,
\acrfull{lz77}~\cite{ziv1977universal} and
\acrfull{lz78}~\cite{ziv1978compression} are well-known algorithms, where
\acrshort{lz77} maintains a sliding window as a dictionary during compression.
There are several variations of \acrshort{lz77} and \acrshort{lz78}, for
instance, \acrfull{lzw}~\cite{sadler2006data},
\acrfull{lzss}~\cite{storer1982data}, \acrfull{lzo}~\cite{lzocite} etc. In order
to suit these algorithms to connected objects and data streams, many
improvements were proposed, such as
\acrfull{a-lzss}~\cite{pope2018accelerometer} which combines \acrshort{lzss} and
Huffman coding to compress accelerometer data, and
\acrfull{s-lzw}~\cite{sadler2006data}, an algorithm which adapts \acrshort{lzw}
to sensor nodes.

Lossy compression methods are particularly suitable for sensor data
streams, because measured sensor data intrinsically involves noise and
measurement errors, which can be treated as a configurable tolerance for a
lossy compression algorithm~\cite{li2018multi}. Thus, in this thesis, we
only focus on lossy compression methods.

Resource-intensive lossy compression algorithms such as the ones based on
polynomial interpolation, discrete cosine and Fourier transforms, or
auto-regression methods~\cite{lu2010optimized} are not well-suited for connected
objects, due to the limited memory available on these systems (typically a few
KB), and the energy consumption associated with CPU usage~\cite{li2018multi}.
Instead, compression algorithms need to find a trade-off between reducing
network communications and increasing memory and CPU usage. As discussed
in~\cite{zordan2014performance}, linear compression methods provide a very good
compromise between these two factors, leading to substantial energy
reduction~\cite{li2018multi}. In this chapter, we will review several lossy
compression methods in Section~\ref{sec:lossy}.
