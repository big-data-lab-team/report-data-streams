\chapter{Conclusion}


\section{Summary}


IoT has been widely popularized in recent years and generates a huge amount of
streaming data. However, the limitations of hardware create challenges. For
instance, connected devices lack enough memory to store all the streaming data,
the micro-controller has trouble with methods that have high computational
complexity, and the battery-based devices cannot have a long lifetime without
recharging or changing the battery. Energy consumption is an urgent problem in
IoT, especially in battery-powered devices, a large amount of computing and the
transmission will drain the battery quickly.

In this thesis, we aimed to find a summarization method which can be fitted in
connected objects and can process multi-dimensional streams to reduce the number
of data transmitted. In that case, we can save energy during transmission,
thereby extending the work time of devices.

Data compression is one of the techniques in data summarization, representing
original data by a compact version and keeping all information (lossless
compression) or retaining the information in certain accuracy. We introduce a
few general compression methods in this thesis and we finally select the
\acrfull{ltc} algorithm for our experiments. \acrshort{ltc} is a lossy
compression method and predicts data by a piece-wise linear function of time,
but it guarantees that the reconstruct accuracy in $\epsilon$ ($\epsilon$ is the
error bound we assigned). 

However, the original \acrshort{ltc} was only available for 1D streaming data.
Thus, we formalize the description of original \acrshort{ltc} and extend
\acrshort{ltc} to dimension $n$. Moreover, LTC n-dimension can work with
different norms, but its compression result and costs depend on the norm used.
In the experiments, we collected 2 biceps curl data-sets which have different
sizes. By using these data sets, we validate the performance of LTC n-dimension
for Infinity and Euclidean norms in terms of compression ratio, memory usage,
and processing time.
% \english{and a comparison of compression ratio, memory usage
% and processing time between infinity norm and Euclidean norm is presented with
% using these data-sets} 
\todo{sentence is too long}.

Then we implemented the LTC n-dimension algorithm into Neblina to measure the
compression ratio of LTC n-dimension algorithm and energy consumption
(compression and transmission) when human is walking or running. From the
result, through using LTC n-dimension algorithm, Neblina can at least save 10\%
energy when the error bound $\epsilon$ is 48.8g. Additionally, in order to test
the compression result of LTC n-dimension, we compared the LTC n-dimension
algorithm with polynomial regression compression algorithm in different
dimension, and we found that LTC n-dimension for Infinity and Euclidean norms
outperform polynomial regression method in terms of compression ratio,
regardless of the dimension or degree of regression. The LTC n-dimension has
been deployed in Motsai's Neblina during my 4 months internship from September
to December 2018, it can work for n-dimensional streams and it can also be used
in Infinity and Euclidean norms.

\section{Limitations}


The main limitation of the LTC n-dimension algorithm is memory usage. In
Euclidean norm, LTC n-dimension can be considered as the intersection of
n-balls, but it is difficult to check whether there is an intersection among
n-balls. The Helly's theorem is the best algorithm we could find,
however, this algorithm is too complex to use in the connected objects which
have limited resource.  We provide a method that utilizes plane sweep and
bisection to determine if a set of n-balls have a public intersection.

Our method has a $O(n\times (\log{2\epsilon})^{d-1})$ complexity,
where $n$ is the number of data points we have seen so far, $\epsilon$ is the
error tolerance and $d$ is the dimension of the stream, but it requires a lot of
memory space and processing time. 

From Section~\ref{sec:experiment2-ltc}, the tolerable latency at 200Hz is
5-ms, the process of current data point must be finished before receiving new
data point. We have to make sure the maximum latency is smaller than tolerable
latency. Thus, the LTC n-dimension cannot work in a high transmission rate, such
as 800Hz. In addition, in order to reduce the memory usage of LTC n-dimension
for the Euclidean norm, we limit the length of the set of n-balls $S$, but it
might reduce the compression ratio. Thus, a better algorithm is needed to deal
with the memory usage and processing time of the intersection check in the
Euclidean norm.


\section{Future work}

In the next step, we would like to find algorithms that have lower time and
space complexity to handle the intersection check. We will review other
compression algorithms, attempting to achieve more energy savings. Moreover, the
lossless algorithm is necessary sometimes, typically in e-health applications.
So a framework which can support both efficient lossless and lossy algorithm and
can work with the multi-dimension stream is also one of the research direction
in our future work.
