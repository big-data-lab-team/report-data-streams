\chapter{Conclusion}


\section{Summary}


IoT has been widely popularized in recent years and generates large amounts of
streaming data. However, hardware limitations of connected devices create
challenges: connected devices lack enough memory to store streaming data,
micro-controllers can hardly run methods that have high computational
complexity, and battery-based devices are limited in lifetime. Energy
consumption is an urgent problem in IoT, especially in battery-powered devices,
as computing and network transmission drain the battery quickly.

In this thesis, we aimed to find a summarization method which can be fitted
in connected objects and can process multi-dimensional streams to reduce
the number of data transmitted. Data compression is one of the techniques
in data summarization, representing original data by a compact version and
keeping all information (lossless compression) or retaining information at
certain accuracy (lossy compression). We introduced a few general
compression methods in this thesis and we finally selected the
\acrfull{ltc} algorithm. \acrshort{ltc} is a lossy compression method that
approximates data streams by a piece-wise linear function of time. It
guarantees that the reconstruction error remains lower than a user-assigned
error bound $\epsilon$. 

However, the original \acrshort{ltc} method was only available for 1D data
streams. Thus, we provided a formulation of \acrshort{ltc} in dimension $n$, and
the corresponding implementation. LTC can work with different norms in dimension
$n$, but its compression result and costs depend on the norm used. In the
experiments, we collected 2 biceps curl data-sets of different sizes. By using
these data sets, we validate the performance of LTC n-dimension for Infinity and
Euclidean norms in respect of compression ratio, memory and processing time

Then, we implemented the LTC n-dimension algorithm into Neblina, the connected
device platform developed by our Motsai partner, to measure the compression
ratio and energy consumption of LTC in dimension $n$, on a dataset of human walk
and run. Results showed that with our n-dimensional implementation of
\acrshort{ltc}, Neblina can save at least 10\% of energy when the error bound
$\epsilon$ is 48.8 mg. Additionally, we compared \acrshort{ltc} in dimension $n$
to polynomial regression in different dimensions. We found that LTC in dimension
$n$ for Infinity and Euclidean norms outperform polynomial regression method in
terms of compression ratio, regardless of the dimension or degree of regression.
Our extension of \acrshort{ltc} in dimension $n$ has been deployed in Motsai's
Neblina during my 4-month internship from September to December 2018.

\section{Limitations}


The main limitation of our current \acrshort{ltc} implementation in
dimension $n$ is memory usage. Our formulation of \acrshort{ltc} in
dimension $n$ boils down to an intersection test between n-balls, however,
with the Euclidean norm this test is difficult. The Helly's theorem is the
best algorithm we could find for this purpose, but it is still too complex
for resource-constrained environments. We provided a method that utilizes
plane sweep and bisection to determine if a set of n-balls intersect.

Our method has a $O(n\times (\log{2\epsilon})^{d-1})$ time complexity, where $n$
is the number of data points we have seen so far since the last transmission,
$\epsilon$ is the error tolerance and $d$ is the dimension of the stream, but it
requires extra memory to save n-balls between two transmissions. Even though we
bound $n$ to 200, it still costs 4.6 KB to compress the long bicep curl data set
in our first experiment (see Table~\ref{tabl:results-validation-d}).
% \TG{you should be more accurate than ``a lot''. You should also say 
% that you bound $n$ to 200 (?) in practice}. 

In addition, compressing data in connected devices causes transmission latency
since it needs processing time. In our case, we have to make sure the maximum
latency is smaller than the sampling rate of sensors, because the process of
current data point must be finished before receiving new data point. For
instance, in Section~\ref{sec:experiment2-ltc}, the sampling rate of
accelerometer sensor is 200 Hz which means a new data point comes each 5-ms,
thus the maximum latency must lower than 5-ms. From
Table~\ref{table:results-energy}, when sampling rate is 200Hz, the latency with
our method is lower than the 5-ms tolerable latency. But LTC n-dimension for
Euclidean norm cannot work in high sampling rate, such as 800Hz, corresponding
1.25-ms tolerable latency.

A better algorithm would be needed to deal
with the memory usage and processing time of the intersection test with the
Euclidean norm.


\section{Future work}

In our future work, we would like to find algorithms that have lower time and
space complexity to handle the intersection check. We will review other
compression algorithms, attempting to achieve more energy savings. Moreover, 
lossless algorithms are necessary sometimes, typically in e-health applications. 
A framework which can support both efficient lossless and lossy algorithm and
can work with the multi-dimension stream is also one of the research
direction in our future work.
