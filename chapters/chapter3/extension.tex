\section{Preliminary comments}

We note that the formulation of LTC 
in~\cite{schoellhammer2004lightweight} relies on the intersection of 
\emph{convex cones} in dimension $n+1$. For $n=1$, it corresponds to 
the intersection of triangles, which can efficiently be computed by 
maintaining boundary lines, as detailed previously \TG{Refer to specific chapterr and section}. In higher 
dimension, however, cone intersections are not so straightforward to compute, due 
to the fact that the intersection between cones may not be a cone.

To address this issue, we formulate LTC as an intersection test between
\emph{n-balls}, that is, segments for $n=1$, disks for
$n=2$, etc. N-balls are defined from the \emph{norm} used in
the vector space of data points. For $n=1$, the choice of the norm does
not really matter, as all p-norms and the infinity norm are identical.
In dimension $n$, however, norm selection will be critical.

\section{Algebraic formulation of LTC}

\subsection{Definitions}

\TG{This now refers to notations presented 10 pages before. You should refer to it at the very least.}
Let $(u_0^k, y_0^k) \in \mathbb{R}^{n+1}$ be the latest transmitted point. For convenience, all the subsequent points will be
expressed in the orthogonal space with origin $(\tau_k, \xi_k)$. We denote by $(v_j, z_j)_{j \in \llbracket 0, m \rrbracket}$ such points:
\begin{equation*}
\forall j \leq m,\  (v_j, z_j) = (u_j^k - \tau_k, y_j^k - \xi_k)
\end{equation*}
Let $\mathcal{B}_j$ be the ball of $\mathbb{R}^n$ of centre $\frac{v_1}{v_j}z_j$ and radius
$\frac{v_1}{v_j}\epsilon$:
\begin{equation*}
\mathcal{B}_j = \left\{ z \in \mathbb{R}^n,\norm{z-\frac{v_1}{v_j}z_j} \leq\frac{v_1}{v_j}\epsilon \right\}
\end{equation*}
Note that $v_1$ is defined as soon as one point is received after the last
transmission.

\subsection{LTC property}

We define the \emph{LTC property} as follows:
\begin{equation*}
\exists z \in \mathbb{R}^n, \ \forall j \in \llbracket 1, m \rrbracket, \norm{\frac{v_j}{v_1}z-z_j} \leq
\epsilon.
\end{equation*}
The original LTC algorithm ensures that the LTC property is
verified between each transmissions. Indeed, all the data points
$z$ such that $(v_1, z)$ is between the high line and the low line
verify the property. Line 13 in Algorithm~\ref{algo:ltc} guarantees that
such a point exists.

The LTC property can be re-written as follows:
\begin{equation*}
\exists z \in \mathbb{R}^n, \ \forall j \in \llbracket 1, m \rrbracket, \norm{z-\frac{v_1}{v_j}z_j} \leq
\frac{v_1}{v_j}\epsilon
\end{equation*}
that is:
\begin{equation}
\bigcap_{j=1}^m \mathcal{B}_j \neq \O
\label{eq:ltc-property}
\end{equation}
Note that $(\mathcal{B}_j)_{j \in \llbracket 1, m \rrbracket}$ is a sequence
of n-balls of strictly decreasing radius, since $v_j > v_1$.

\section{Algorithm}

The LTC algorithm generalized to dimension $n$ tests that the LTC 
property in Equation~\ref{eq:ltc-property} is verified after each reception of a data 
point. It is written in Algorithm~\ref{algo:general-ltc}.
\begin{algorithm}
\begin{algorithmic}[1]
\Input
   \Desc{$(u^k_j, y^k_j)$}{$\quad \quad $Received data stream}
   \Desc{$\epsilon$}{$\quad \quad$Error bound}
\EndInput
\Output
   \Desc{tr}{Transmitted points}
\EndOutput

\State tr = ($\tau, \xi$) = ($u^0_0, y^0_0$) \Comment{Last transmitted point}
\State k = 0 ; j = 0
\While{True}
    \State j += 1
    \State ($v_j, z_j$) = ($u_j^k - \tau, y_j^k - \xi$)
    \If{$\bigcap_{l=1}^j{\mathcal{B}_l} = \O$}
        \State Pick $z$ in $\bigcap_{l=1}^{j-1}{\mathcal{B}_j}$ \Comment{Transmit point}
        \State tr = ($\tau$, $\xi$) = ($u^k_{j-1}, z$)
        \State k += 1
        \State j = 1
    \EndIf
\EndWhile
\end{algorithmic}
\caption{Generalized LTC.}
\label{algo:general-ltc}
\end{algorithm}

\section{Intersections of n-balls}

Although Algorithm~\ref{algo:general-ltc} looks simple, one should not
overlook the fact that there is no good general algorithm to test
whether a set of n-balls intersect. The best general algorithm we could find
so far relies on Helly's theorem which is formulated as follows~\cite{helly1923mengen}:
\begin{theorem}
Let $\left\{ X_i \right\}_{i \in \llbracket 1, m \rrbracket}$ be a collection of convex subsets of $\mathbb{R}^n$. If the intersection of every $n+1$
subsets is non-empty, then the whole collection has an non-empty intersection.
\end{theorem}
\noindent This theorem leads to an algorithm of complexity ${m \choose
n+1}$ which is not usable in resource-constrained environments such as connected objects.

The only feasible algorithm that we found is norm-specific. It
maintains a representation of the intersection
$\bigcap_{j=1}^{m}{\mathcal{B}_j}$ that is updated at every iteration.
The intersection tests can then be done in constant time. However,
updating the representation of the intersection may be costly
depending on the norm used. For the infinity norm, the representation
is a rectangular cuboid which is straightforward to update by
intersection with an n-ball.
For the Euclidean norm, the representation is an arbitrary volume,
which is more costly to maintain.

\section{Effect of the norm}

As mentioned before, norm selection in $\mathbb{R}^n$ has a critical
impact on the compression error and ratio. To appreciate this effect,
let us compare the infinity norm and the
Euclidean norm in dimension 2. By comparing the unit disk to a
square of side 2, we obtain that the compression ratio of a random stream would
be $\frac{4}{\pi}$ times larger with the infinity norm than with the 
Euclidean norm. In 3D, this ratio would be $\frac{6}{\pi}$. Conversely, 
a compression error bounded by $\epsilon$ with the infinity norm 
corresponds to a compression error of $\frac{\epsilon}{\sqrt{n}}$ with 
the Euclidean norm. \TG{This could be clarified, e.g., through a figure.} Unsurprisingly, the
infinity norm is more tolerant than the Euclidean norm.

It should also be noted that using the infinity norm in $\mathbb{R}^n$ 
boils down to the use of the 1D LTC algorithm independently in each 
dimension, since a data point will be transmitted as soon as the linear 
approximation doesn't hold in any of the dimensions. For the Euclidean 
norm, however, the multidimensional and multiple unidimensional 
versions are different: the multiple unidimensional version 
behaves as the infinity norm, but the multidimensional version is more 
stringent, leading to a reduced compression rate and error.

To choose between the multidimensional implementation and multiple 
unidimensional ones, we recommend to check whether the desired 
error bound is expressed independently for every sensor, or as an aggregate error between them.
The multidimensional version is
more appropriate for multidimensional sensors, for instance 3D 
accelerometers or 3D gyroscopes, and the multiple unidimensional 
version is more suitable for multiple independent sensors, for 
instance a temperature and a pressure sensor.
%~ For instance, in case of a 3D accelerometer, if the error 
%~ is expressed simply as 
%~ ``10~mg", then the multidimensional version should be chosen because it 
%~ will guarantee that the norm of the 3D acceleration vector will be 
%~ reconstructed with an error less than 10~mg. Conversely, if the error is 
%~ specified as ``10~mg in x, 
%~ 10~mg in y and 10~mg in z", then the multiple unidimensional version 
%~ should be used.
