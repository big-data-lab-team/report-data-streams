\section{LTC n-dimension compare with Regression}
In previous experiments, we knows that LTC has good performance on dealing with
the data set which has long constant and same differences. If high-order
regression method would work better in real activity data sets. In this
paragraph , we compare Polynomial Regression with LTC in different dimension. 

% need  a graph for polynomial regression with different p,  by using 200Hz walking and running
%data set, also need a table to show the RMSE and compression ratio, Max error.

\subsection{Implementation of Polynomial Regression}
We implement polynomial regression method to compress the accelerometer data
($t$, $a.x$, $a.y$, $a.z$). In the data-set, we process polynomial regression
with the relationship between independent variable $t$(Time-stamp) and dependent
variables $a.x$, $a.y$ and $a.z$. For tolerance checking, we use infinity norm
and euclidean norm. The implementation needs much memory to record data points,
and some CPU time to compute coefficients.

\begin{figure}
  \centering
\includegraphics[width=0.7\textwidth]{figures/point-in-chord.png}
\caption{The point selected to determine the direction}
\label{fig:compare_point}
\end{figure}

We use Polynomial Regression method on previous walking and running data set. Figure~\ref{fig:poly-regression} shows the result.

\begin{figure*}
\centering
\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=0.3\columnwidth]{figures/regression-p3-in-x.pdf}
\includegraphics[width=0.3\columnwidth]{figures/regression-p3-in-y.pdf}
\includegraphics[width=0.3\columnwidth]{figures/regression-p3-in-z.pdf}
\caption{Regression degree=3 with Infinity norm}
\end{subfigure}
{\footnotesize}

\centering
\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=0.3\columnwidth]{figures/regression-p3-Eu-x.pdf}
\includegraphics[width=0.3\columnwidth]{figures/regression-p3-Eu-y.pdf}
\includegraphics[width=0.3\columnwidth]{figures/regression-p3-Eu-z.pdf}
\caption{Regression degree=3 with Euclidean norm}
\end{subfigure}

\centering
\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=0.3\columnwidth]{figures/regression-p5-in-x.pdf}
\includegraphics[width=0.3\columnwidth]{figures/regression-p5-in-y.pdf}
\includegraphics[width=0.3\columnwidth]{figures/regression-p5-in-z.pdf}
\caption{Regression degree=5 with Infinity norm}
\end{subfigure}
{\footnotesize}

\centering
\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=0.3\columnwidth]{figures/regression-p5-Eu-x.pdf}
\includegraphics[width=0.3\columnwidth]{figures/regression-p5-Eu-y.pdf}
\includegraphics[width=0.3\columnwidth]{figures/regression-p5-Eu-z.pdf}
\caption{Regression degree=5 with Euclidean norm}
\label{fig:poly-regression}
\end{subfigure}

\end{figure*}

\subsection{Results of Comparison}
In our implementation, for a regression model which has N-degree and
M-parameters we need M*(N+1) coefficients and extra time-stamp used to record
where should the model stop. In the Table , It obvious that LTC n-dimension work
better than Polynomial regression, because it need more bytes to save the
coefficients. In our case, coefficients are recorded as float points which would
use 4 bytes per each. The total number of bytes needed to represent original
data is (N+1)*M*4+4 (Int32 for Time-stamp). Polynomial regression using more
processing time to calculate coefficients and build model when each new data
point comes.
\begin{table}[]
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
           & \multicolumn{2}{c|}{LTC} & \multicolumn{2}{c|}{Regression degree =3} & \multicolumn{3}{l|}{Regression degree =5}\\ \hline
Dimension & Infinity   & Euclidean   & Infinity    & Euclidean     & Infinity      & \multicolumn{2}{l|}{Euclidean} \\ \hline
1          & 90.1\%     & 90.1\%      & 89\%        & 89\%          & 88.96\%       & \multicolumn{2}{l|}{88.96\%}   \\ \hline
2          & 89.6\%     & 88.7\%      & 83.35\%     & 83.35\%       & 83.55\%       & \multicolumn{2}{l|}{83.1\%}    \\ \hline
3          & 88.9\%     & 87.6\%      & 80.24\%     & 78.68\%       & 80.8\%        & \multicolumn{2}{l|}{77.96\%}   \\ \hline
\end{tabular}
\caption{Walking}
\end{table}


\begin{table}[]
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
           & \multicolumn{2}{c|}{LTC} & \multicolumn{2}{c|}{Regression degree =3} & \multicolumn{3}{l|}{Regression degree =5}          \\ \hline
Dimension & Infinity   & Euclidean   & Infinity            & Euclidean           & Infinity& \multicolumn{2}{l|}{Euclidean} \\ \hline
1          & 74.7\%     & 74.7\%      & 70.7\%      & 70.7\%        & 71.1\%        & \multicolumn{2}{l|}{71.1\%}    \\ \hline
2          & 70.6\%     & 68.6\%      & 58.3\%      & 57.4\%        & 58.4\%        & \multicolumn{2}{l|}{57.35\%}   \\ \hline
3          & 68.6\%     & 64.4\%      & 51.1\%      & 48\%          & 50.2\%        & \multicolumn{2}{l|}{47.92\%}   \\ \hline
\end{tabular}
\caption{Running}
\end{table}
