\section{LTC n-dimension compare with Regression}

From the previous experiments, we know that LTC has good performance on  walking
and running accelerometer data sets. However as we have seen, the running data
set (Figure~\ref{fig:datasets-2}) looks like it is made of many high-degree
polynomials. We wonder whether high-degree regression compression method
performs better with our accelerometer data sets. In this paragraph, we
implement the Polynomial Regression compression method mentioned
in~\ref{sec:polynomial} and we compare it with LTC in different dimensions.

% need  a graph for polynomial regression with different p,  by using 200Hz
% walking and running data set, also need a table to show the RMSE and
% compression ratio, Max error.

\subsection{Implementation of Polynomial Regression}

We implemented polynomial regression method to compress 3D acceleration data
($t$, $a.x$, $a.y$, $a.z$). In the implementation, we process polynomial
regression compression method on each accelerometer parameter. There are three
polynomial functions between independent variables $t$ (time-stamp) and
dependent variables $a.x$, $a.y$ or $a.z$ (e.g. $t \sim a.x$). In other words,
it means that three regression models shall be transmitted to represent a curve
in 3D space. For tolerance checking, we use infinity norm and Euclidean norm.

As we mentioned in Section~\ref{sec:polynomial}, $M_{j}^{k}$ means the $j^{th}$
polynomial regression model after $k^{th}$ transmission, but to apply this
method on the acceleration data which is 3-dimensional, we need three models for
it, one for each parameter. Assuming $Mx_{j}^{k}$, $My_{j}^{k}$ and $Mz_{j}^{k}$
represent the regression model on each parameter $x$, $y$ and $z$ respectively,
Algorithm~\ref{algo:n-polynomial} shows how polynomial regression is applied on
3D acceleration data. In line 13, the coefficients of three regression models of
parameters $x$, $y$ and $z$ are calculated, and each time a new point arrives
they will be recalculated. The function \texttt{max\_residue()} returns the
maximum error between predicted data and original data. If the maximum residue
is larger than $\epsilon$, then the last regression model that meets error
tolerance and the time-stamp of last data are transmitted as compression result.
\begin{algorithm}
\begin{algorithmic}[1]
\Input
    \Desc{($\chi, t_i$)}{$\quad $Received data stream at time $t_i$}
    \Desc{$\epsilon$}{$\quad $Error bound}
    \Desc{$p$}{$\quad $The order of polynomial function}
\EndInput
\Output
    \Desc{tr}{Transmitted coefficients}
\EndOutput
\State $S$ = $\text{\O}$
\State $k$=0; $j$=0

\While{True}
    \State $S = S \cup (\chi, t_i)$
    \If{$S.length() \geqslant p+1$}
        \State j += 1
        \State ($Mx_{j}^{k}$, $My_{j}^{k}$, $Mz_{j}^{k}$) = model($S$, $p$)    \Comment{Compute coefficients}
        \If{max\_residue($Mx_{j}^{k}$, $My_{j}^{k}$, $Mz_{j}^{k}$, $S$) $> \epsilon$}
            \State tr = ($Mx_{j-1}^{k}$, $My_{j-1}^{k}$, $Mz_{j-1}^{k}$, $t_{i-1}$)
            \State k += 1; j = 0
            \State $S$ = $\chi$
        \EndIf
    \EndIf
\EndWhile

\end{algorithmic}
\caption{Polynomial Regression Algorithm for 3D Accelerometer data}
\label{algo:n-polynomial}
\end{algorithm}

We use the implemented polynomial regression method on previous walking and
running data sets. Figure~\ref{fig:poly-regression-3-degree} and
Figure~\ref{fig:poly-regression-5-degree} shows the reconstructed data. In the
compression process, we need to keep in memory all the data between two
transmissions and update regression models each time a new data point arrives.

\begin{figure*}
\centering
\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=0.3\columnwidth]{figures/regression-p3-in-x.pdf}
\includegraphics[width=0.3\columnwidth]{figures/regression-p3-in-y.pdf}
\includegraphics[width=0.3\columnwidth]{figures/regression-p3-in-z.pdf}
\caption{Regression degree=3 with Infinity norm}
\end{subfigure}

\centering
\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=0.3\columnwidth]{figures/regression-p3-Eu-x.pdf}
\includegraphics[width=0.3\columnwidth]{figures/regression-p3-Eu-y.pdf}
\includegraphics[width=0.3\columnwidth]{figures/regression-p3-Eu-z.pdf}
\caption{Regression degree=3 with Euclidean norm}
\end{subfigure}

\caption{Reconstructed data by using 3-degree Regression compression method}
\label{fig:poly-regression-3-degree}
\end{figure*}

%-------------------------------------------------------%

\begin{figure*}
\centering
\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=0.3\columnwidth]{figures/regression-p5-in-x.pdf}
\includegraphics[width=0.3\columnwidth]{figures/regression-p5-in-y.pdf}
\includegraphics[width=0.3\columnwidth]{figures/regression-p5-in-z.pdf}
\caption{Regression degree=5 with Infinity norm}
\end{subfigure}

\centering
\begin{subfigure}{\columnwidth}
\centering
\includegraphics[width=0.3\columnwidth]{figures/regression-p5-Eu-x.pdf}
\includegraphics[width=0.3\columnwidth]{figures/regression-p5-Eu-y.pdf}
\includegraphics[width=0.3\columnwidth]{figures/regression-p5-Eu-z.pdf}
\caption{Regression degree=5 with Euclidean norm}
\end{subfigure}

\caption{Reconstructed data by using 5-degree Regression compression method}
\label{fig:poly-regression-5-degree}
\end{figure*}

\subsection{Results of Comparison}
In our implementation, for a regression model with N-degree and M-parameters we
need M*(N+1) coefficients and extra time-stamp to record when the coefficients
are transmitted. In our case, coefficients are recorded as single-precision
floating-point numbers which would use 4 bytes per each. The total number of
bytes needed to represent the original data is $(N+1)\times M\times 4+4$ (4
bytes for recording Time-stamp).

From Table~\ref{table:CR-compare-walking} and
Table~\ref{table:CR-compare-running}, we observe that increasing the dimension
$n$ is detrimental to compression ratio with both n-dimensional LTC and
Polynomial Regression. Moreover, LTC n-dimension gives better compression ratio
than regression method on different dimensions, even though regression method
represents original data with fewer segments (curve segments). It is because
polynomial regression method requires more bytes to transmit the coefficients
while LTC n-dimension just transmits a data point each time. In other words, we
need to represent each regression curves with $(N+1)\times M\times 4+4$ bytes,
but each straight line can be defined by two data points with $2\times(M\times
2+4)$ bytes (2 bytes for each parameter). Assume Polynomial regression and LTC
n-dimension needs $G_P$ curves and $G_L$ lines respectively to represent
original data set. Then we have to transmit $((N+1)\times M\times 4+4)\times
G_P$ bytes by using polynomial regression, and $2\times(M\times 2+4) \times
(G_L-1)$ bytes ($G_L-1$ data points) by using LTC n-dimension because LTC
n-dimension generates connected straight lines.
In our case, the polynomial regression method uses more bytes to represent
original stream than LTC n-dimension. In reality, the compression ratio of these
two methods depends on the data set, the polynomial regression method would give
us higher compression ratio than LTC n-dimension, if the regression model really
fits our data set.
 
% \TG{\#\#This is very surprising, as a straight line is a particular case of polynomial of degree 3 or 5.} 
Besides, similarly to Experiment 2 in Section~\ref{sec:experiment2-ltc}, the
compression ratio is higher for the infinity norm than for the Euclidean and is
higher for the walking than for running. For given degree of polynomial
regression method, the higher degree results in smaller compression ratio for
walking and running in general, but the variations are slight. However, in
Table~\ref{table:CR-compare-walking}, when dimension is 2 and 3, the 5-degree
regression compresses more data then 3-degree regression method in walking data
set for Infinity norm.  In Table~\ref{table:CR-compare-running}, the 5-degree
regression method has higher compression ratio then 3-degree regression for both
norm when dimension equals 1 and for infinity norm when dimension equals 2.

Polynomial regression method needs a lot of memory to save all the data points
between two transmission and long processing time to calculate coefficients and
build model when each new data point comes. Because we implemented polynomial
regression method and LTC n-dimension in different platform, the comparison of
memory usage and processing time between them cannot be measured.

% So, using polynomial regression method needs more memory and processing time
% than n-dimensional LTC for infinity norm, but for Euclidean norm, we need to
% also record n-balls between two transmissions and execute intersection checking. 
% \TG{so does polynomial regression also use more memory and processing than LTC for Euclidean norm?}
%  Polynomial regression uses more processing time \TG{more
% than what?} to calculate coefficients and build model when each new data point
% comes. 

\begin{table}
\begin{subfigure}{\columnwidth}
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
           & \multicolumn{2}{c|}{LTC n-dimension} & \multicolumn{2}{c|}{Regression degree = 3} & \multicolumn{3}{l|}{Regression degree = 5}\\ \hline
Dimension & Infinity   & Euclidean   & Infinity    & Euclidean     & Infinity      & \multicolumn{2}{l|}{Euclidean} \\ \hline
1          & 90.1\%     & 90.1\%      & 89\%        & 89\%          & 88.96\%       & \multicolumn{2}{l|}{88.96\%}   \\ \hline
2          & 89.6\%     & 88.7\%      & 83.35\%     & 83.35\%       & 83.55\%       & \multicolumn{2}{l|}{83.1\%}    \\ \hline
3          & 88.9\%     & 87.6\%      & 80.24\%     & 78.68\%       & 80.8\%        & \multicolumn{2}{l|}{77.96\%}   \\ \hline
\end{tabular}
\caption{Comparison of compression ratios on Walking data set}
\label{table:CR-compare-walking}
\end{subfigure}
\\
\begin{subfigure}{\columnwidth}
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
           & \multicolumn{2}{c|}{LTC n-dimension} & \multicolumn{2}{c|}{Regression degree = 3} & \multicolumn{3}{l|}{Regression degree = 5}          \\ \hline
Dimension & Infinity   & Euclidean   & Infinity            & Euclidean           & Infinity& \multicolumn{2}{l|}{Euclidean} \\ \hline
1          & 74.7\%     & 74.7\%      & 70.7\%      & 70.7\%        & 71.1\%        & \multicolumn{2}{l|}{71.1\%}    \\ \hline
2          & 70.6\%     & 68.6\%      & 58.3\%      & 57.4\%        & 58.4\%        & \multicolumn{2}{l|}{57.35\%}   \\ \hline
3          & 68.6\%     & 64.4\%      & 51.1\%      & 48\%          & 50.2\%        & \multicolumn{2}{l|}{47.92\%}   \\ \hline
\end{tabular}
\caption{Comparison of compression ratios on Running data set}
\label{table:CR-compare-running}
\end{subfigure}
\caption{Results of Comparison}
\end{table}
